# Create Data Pipelines for Continuous Data Export and Import

## Introduction

Oracle Data Pipelines provide a continuous, incremental and fault-tolerant way to export and import data into ADB. With data pipelines, you can quickly and automatically load data into your database such as from your object store, as your ETL jobs and other data sources bring in new, clean data into your object store.

If you are loading data into or exporting out of ADB today, you are likely familiar with the `DBMS\_CLOUD` package that provides the ability to load data into your database from the object store with `DBMS\_CLOUD.COPY\_DATA` or export data to your object store using `DBMS\_CLOUD.EXPORT\_DATA`. You may find yourself performing these operations repeatedly (you may even have scheduled jobs) to work with new data that is flowing into your object store or tables. This new Data Pipeline feature introduces the package `DBMS\_CLOUD\_PIPELINE` to simplify and automate this process, providing a unified solution of scheduled jobs for periodic data load and export of new data files with intuitive configurable knobs, legible troubleshooting outputs and default parallelism for optimal scalability.

The two types of data pipelines available are:

1. **Load Data Pipelines**: Data pipelines used for periodically loading data into the database, from new data files lying in your object store of choice. Some use cases for load pipelines would be:
    - Continuous migration of new on-premise data sets into the database via the object store of choice, using a load pipeline
    - Loading new, incoming real-time analytic data or outputs of an ETL process into the database using a load pipeline, via data files store in the object store

2. **Export Data Pipelines**: Data pipelines used for periodically exporting new, incremental data as results from a table or query in the database, to the object store of choice. An example use case for export pipelines would be:
    - Exporting new time-series style data generated by your application from the database to the object store at periodic intervals

Now that we understand what a data pipeline is in ADB, let's walk through how to create and set up a pipeline, to understand how it works. The steps you will follow to create and use a data pipeline are:

  1. Create a new data pipeline to either load data into the database or export data from the database
  2. Configure your data pipeline by setting the right attributes as it relates to your data
  3. Test that the data pipeline loads or exports some sample data as expected
  4. Start a pipeline to continuously load or export your data

    ![Diagram of the lifecycle of cloud data pipeline](images/pipeline-lifecycle.png " ")

Estimated Time: 10 minutes

### Objectives

In this lab, you will:
* Create a data pipeline
* Configure the data pipeline attributes
* Test the data pipeline
* Start the data pipeline
* Reset the pipeline's state and history

### Prerequisites

- This lab requires completion of the lab **Provision an Autonomous Database** found in the Contents menu on the left.

## Task 1: Create a data pipeline

Begin by creating a data pipeline to either load data or export data continuously.

1. Return to Database Actions and click **SQL** to open a **SQL Worksheet**. Click the **Copy** button to copy this code snippet into the worksheet and run this snippet for creating a data pipeline to load data. **Note the pipeline\_type parameter**:

    ```
    <copy>
    BEGIN
     DBMS_CLOUD_PIPELINE.CREATE_PIPELINE(
        pipeline_name => 'MY_FIRST_PIPELINE',
        pipeline_type => 'LOAD',
        description   => 'Load employee data from object store into a table'
    );
    END;
    /
    </copy>
    ```

## Task 2: Configure the data pipeline attributes

Next, you will set the right attributes for the data pipelines, such as the type of data files (for example JSON, CSV) and the location where the data files will lie (for example an object store bucket or file folder), as well as create the destination table in the database that your pipeline will load data into.

1. In this example, we are considering employee data so let's create the destination table **EMPLOYEE**:

    ```
    <copy>
    CREATE TABLE EMPLOYEE (name VARCHAR2(128), age NUMBER, salary NUMBER);
    </copy>
    ```

2. **DO I REFERENCE THE LABS IN LOAD AND ANALYZE TO CREATE BUCKET WITH DATA FILES IN IT AND A CREDENTIAL PARAMETER TO ACCESS IT FROM THIS WORKSHOP'S DATABASE? OR DO I COPY/PASTE THE TASKS FROM THAT WORKSHOP INTO HERE?**




## Task 3: Test the data pipeline

Introductory text

1. text

## Task 4: Start the data pipelines

Introductory Text

1. text

## Task 5: Optionally reset the pipeline's state and history

Introductory Text

1. text

Please [proceed to the next lab](#next).

## Learn More

The following link provides more information about Oracle Data Pipelines:

- [Oracle documentation on data pipelines](https://docs.oracle.com/en/cloud/paas/autonomous-database/adbsa/pipelines-about.html#GUID-333ABC83-57FA-46CE-887A-F0555461ED80)

## Acknowledgements

* **Author** - Rick Green, Principal Developer, Database User Assistance
* **Contributor** Nilay Panchal, Principal Product Manager, Autonomous Database
* **Last Updated By/Date** - Rick Green, January 2023

Data about movies in this workshop were sourced from Wikipedia.

Copyright (C) Oracle Corporation.

Permission is granted to copy, distribute and/or modify this document
under the terms of the GNU Free Documentation License, Version 1.3
or any later version published by the Free Software Foundation;
with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
A copy of the license is included in the section entitled [GNU Free Documentation License](files/gnu-free-documentation-license.txt)
